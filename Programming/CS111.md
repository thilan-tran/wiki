---
title: "CS111"
subtitle: "Xu"
date: "Winter 2020"
mainfont: Libertinus Serif
monofont: Iosevka
fontsize: 14pt
geometry: margin=2cm
toc: true
documentclass: extarticle
header-includes: |
  \usepackage{caption}
  \hypersetup{colorlinks=true,linkcolor=black,urlcolor=myblue}
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
  \usepackage{xcolor}
  \definecolor{mygray}{HTML}{A6A5A2}
  \definecolor{mygreen}{HTML}{98C379}
  \definecolor{myblue}{HTML}{61AFEF}
  \definecolor{mycyan}{HTML}{56B6C2}
  \definecolor{myorange}{HTML}{E5C07B}
  \definecolor{myred}{HTML}{E06C75}
  \definecolor{mypurple}{HTML}{AE81FF}
  \usepackage{listings}
  \lstset{
  language=c++,
  basicstyle=\ttfamily,
  commentstyle=\color{mygray}\textit,
  keywordstyle=\color{mycyan}\bfseries,
  identifierstyle=\color{mygreen},
  stringstyle=\color{myorange},
  directivestyle=\color{mypurple},
  numberstyle=\small\color{mygray},
  rulecolor=\color{mygray},
  captionpos=t,
  title=\lstname,
  columns=fullflexible,
  lineskip=2pt,
  breakatwhitespace=false,
  breaklines=true,
  extendedchars=true,
  keepspaces=true,
  showspaces=false,
  showtabs=false,
  tabsize=2,
  frame=trbL,
  numbersep=9pt,
  stepnumber=2,
  literate=%
  {0}{{{\color{mypurple}0}}}1
  {1}{{{\color{mypurple}1}}}1
  {2}{{{\color{mypurple}2}}}1
  {3}{{{\color{mypurple}3}}}1
  {4}{{{\color{mypurple}4}}}1
  {5}{{{\color{mypurple}5}}}1
  {6}{{{\color{mypurple}6}}}1
  {7}{{{\color{mypurple}7}}}1
  {8}{{{\color{mypurple}8}}}1
  {9}{{{\color{mypurple}9}}}1
  {+}{{{\color{myred}+}}}1
  {-}{{{\color{myred}-}}}1
  {>}{{{\color{myred}>}}}1
  {<}{{{\color{myred}<}}}1
  {=}{{{\color{myred}=}}}1
  {\ *\ }{{{\color{myred}\ *\ }}}1
  {\ /\ }{{{\color{myred}\ /\ }}}1,
  backgroundcolor=\color{gray!10}}
  \usepackage{microtype}
---

\newpage{}

# CS111
***

## Introduction to OS
***

- **Von Neumann** model of computing:
  - when a program is run, the processor repeatedly *fetches* an instruction from memory, *decodes* it, and *executes* it

- [OS Principles](http://htmlpreview.github.io/?https://github.com/markkampe/Operating-Systems-Reading/blob/master/principles.html)
- *complexity* management principles:
  - layered structure and hierarchical decomposition
  - modularity and functional encapsulation
  - appropriately abstracted interfaces and information hiding
  - powerful abstractions
  - interface contracts
  -  progressive refinement
- *architectural* paradigms:
  - mechanism/policy separation
  - indirection, federation, and deferred binding
  - dynamic equilibrium
  - criticality of data structures

- the **operating system (OS)** is in charge of making the system operates correctly and efficiently in an *abstracted*, easy-to-use manner:
  - acts as the software layer between hardware and higher level applications, abstracts and hides the low level details eg. hardware and ISAs
  - uses technique of **virtualization** to transform a *physical* resource into a generalized, easy-to-use *virtual* form
    - OS thus also known as **virtual machine**
    - eg. in order to virtualize memory, each running program seems to have its own private memory, instead of sharing the actual physical memory
  - provides services through interfaces and **system calls** in a **standard library** that users can use
  - acts as a **resource manager** to manage resources such as the CPU, memory, and disk
    - eg. abstracts physical memory *disks* as *files*
  - virtualizes the CPU, ie. turning a small number of CPUs into infinite CPUs that can run many programs at once
    - this **concurrency** can lead to different problems for the OS itself as well as **multi-threaded** programs that require certain mechanisms to solve
  - handles data **persistence** with the file system and I/O
  - deals with **drivers** and coordination with external devices
- basic OS goals include *abstraction*, minimizing *overhead* (ie. in time or space), providing *protection* and *isolation* between applications, and high *reliability*
- original role has changed over time from harnessing hardware, shielding applications from hardware, to providing an ABI platform, to acting as a "traffic cop"
  - over time, different OSs have converged, since they are so difficult to *maintain*
  - applications have to *choose* to support an OS
  - new OSs must have some clear advantages over alternatives

- **instruction set architectures (ISAs)** are a computer's lowest-level supported instructions/primitives
  - many different, incompatible ISAs
    - thus OS also responsible for running on *different* ISAs and abstracting them
  - only OS/kernel can work with the *priveleged* ISA, but standard ISA is accessible by all
- OS abstracts ISAs into a set of management and abstraction *services* accessible through a **system call interface**
- system calls may be further abstracted into an **application binary interface (ABI)**
- the OS code is *unique* from application code:
  - eg. applications should not be able to read from anywhere on disk
  - thus, OS should distinguish between:
    - **system calls** that require formal hardware instructions to use the OS (ie. jumps into the priveged *kernel* mode and raises hardware privelege level)
    - **procedure calls** that are provided as a library and are accessible in the *user* mode

- **resources** have different types:
  - **serial** - used by multiple clients, one at a time, eg. printer
    - serial multiplexing
    - need *graceful* transitions when switching between clients, mechanisms for exclusive use, cleanup of incomplete operations, etc.
  - **partitonable** - divided into disjoint pieces for multiple clients, eg. memory
    - spatial multiplexing
    - need access control for containment and privacy, transitions
  - **shareable** - used by multiple concurrent clients, eg. OS shared by multiple processes
    - no need for transitions, no unique state for particular client

- the OS should handle *abstractions* in order to:
  - *encapsulate* implementation details
  - provide more *convenient* and powerful behavior
- core OS abstractions include:
  - processor, memory, and communications abstractions
- **process abstractions**:
  - the source code of a program specifies a program's behavior
  - when running as a process, the stack, heap, and register contents form its environment
    - must be independent from other processes
  - but the CPU thus must be shared across many processes:
    - CPU **schedulers** share the CPU among those processes
    - **memory management** hardware and software gives the illusions of full exclusive memory use for each process
    - **access control** mechanisms to keep processes independent
- **memory abstraction**:
  - at a low level, there are many different related data storage resources:
    - variables, chunks, files, database records, messages
    - all with unique, pecuilar characteristics
  - OS must abstract these physical devices to create ones with consistent, more *desirable* properties:
    - **persistence**
    - user desired **size**
    - **coherency** (reads reflect writes) and **atomicity** (full writes and reads)
    - **latency**
  - OS will thus:
    - have a *thorough* file system component
    - *optimize* caching
    - have sophisticated organizations to handle *failures*
- **communications abstractions**:
  - networks and interprocess communication mechanisms
  - different from memory:
    - highly *variable* performance
    - *asynchronous*
    - *complications* from working with remote machines

## Services
***

- a **service** is a provided functionality
  - in an OS, the client of its services are applications
- decomposed into:
  - **interface** - the *specification* of the service, ie. description of pre- and post-conditions
  - **implementation** of the interface
- main types of OS services include:
  - **CPU/memory** - processes, threads, virtual addresses, lowest latency memory
  - **persistent storage** - disks, files, and file systems, higher latency memory
  - **I/O** - terminals, windows, sockets, networks, signals (interrupts), highest latency memory
  - note each service family can be associated with a memory latency class
    - when CPU is waiting for a process's slow memory access, may use **context switching** to switch to a different process
- *higher* level OS services:
  - used by clients
  - cooperating parallel processes
  - security, eg. authentication and encryption
  - providing a UI
- *lower* level OS services:
  - not as visible
  - hardware handling
  - software updates, config registry
  - resource allocation and scheduling
  - network and protocols

### Delivery

- the OS *delivers* these various services at different layers:
  - **subroutines** (functions), eg. `malloc()` provided by `libc` library (implementation uses a system call)
    - implemented at higher layers to provide richer operations
    - simplest access, just call subroutines
      - at a lower level: push parameters, jump, return values in registers
    - **pros**:
      - fastest, can be implemented to use the fewest system calls, eg. buffered read and writes
      - can bind implementations at runtime
    - **cons**:
      - services implemented in same virtual address space *associated* with the running program
      - limited to a language
      - can't use priveleged instructions
    - provided in **libraries**
    - **pros**:
      - code reuse, single copy, encapsulates complexity
      - many bind-time options: *static* (included at link time), *shared* (mapped into address space at exec time), *dynamic* (choose at load time)
  - **system calls**
    - forces an entry into the OS, implementation uses privilege kernel
    - **pros**:
      - can use previleged resources and operations
      - can communicate with other processes
    - **cons**:
      - very specific use cases, eg. viewing status of a page table
      - slower, the process may have to switch to a priveleged kernel mode
      - requires traps and hardware to perform
  - send **messages** to software that performs services
    - used in distributed systems, exchange messages with a server
    - **pros**:
      - server can be anywhere
      - service is highly scalable and available
    - **cons**:
      - slowest method

### Interfaces

- standardized **interfaces** in software are inspired by the concept of *interchangeable* parts
  - ie. every part has specifications that allow any collection of parts to be assembled together
    - *pros*: standards end up being extensively reviewed, platform-neutral, and clear and complete
    - *cons*: standards constrain possible implementations and consumers, and can be hard to evolve, leading to obsolescence
    - **proprietary** interfaces are controlled by a single organization, which puts the burden on the org to develop it
    - **open standards** are controlled by a consortium of providers, which may lead to reduced freedom and competitive advantage
- using interfaces for the components of a complex system architecture allows for modularity and independent designs and implementations
  - but interfaces and implementations should be defined *independently*
- an interface's specifications is a **contract** between developers and the implementation providers
  - if this contract is broken, programs are no longer portable and solving issues becomes more complex
  - **backwards compatibility** can still be maintained with some strategies:
    - **interface polymorphism** for different versions of a method with unique signatures
    - **versioned interfaces** with micro, minor, or major releases

- an **application programming interface (API)**:
  - defines *subroutines*, what they do, and how to use them
  - ie. a source level interface, helps write programs for the OS
  - includes discussion of signatures, options, return values and errors
  - eg. in a simple "Hello World", two system calls are made using their respective **API**s:
    - `write(fd, p, num)` writes `num` bytes from the address at `p` to the file descriptor `fd`
    - `exit_group(code)` exits the prgram with exit code `code`
- an **application binary interface (ABI)**:
  - *binds* an API to an ISA
    - applications work *above* the ABI, while the kernel and machine level operations lie *under* the ABI
  - ie. a binary interface specifying the DLLs, data formats, calling sequences, linkage conventions
    - help install binaries on the OS
  - describes the *machine language* instructions and convention to call routines for a specific ISA
    - eg. the binary representation of data types, stack-frame structure, register conventions
  - usually used by the compiler, linker, loader, and OS
  - eg. in the above "Hello World", the system call **ABI** for Linux x86-64 consists of the assembly instruction `syscall`
    - where the register `rax` holds the system call number, and registers `rdi`-`r9` hold the 6 possible arguments

## Programs and Processes
***

- general software file classes:
  - **source** files are editable text files in a programming language
  - **object modules** are relocatable sets of compiled or assembled instructions from source files
  - **libraries** are collections of object modules, source files can fetch functions from them
    - order in which libraries are searched can matter
  - **load modules** are complete programs that can be loaded into memory and executed into CPU

- software generation tool chain:
  - **compiler** produces lower-level assembly language code from source modules
  - **assembler** creates an object module in mostly machine language code from assembly language files
    - handles lower-level operations including CPU initialization, traps/interrupts, sychronization
    - however, some functions and data may not yet be present and not all memory addresses are finalized
      - ie. references and addresses can only be relative to the start of the module addresses
  - **linkage editor** reads a set of object modules, places them into a virtual address space (VAS), *resolves* external references in the VAS, and finalizes all symbol addresses
    - creates an executable load module
    - **resolution** searches through specified libraries to find object modules that satisfy unresolved references
    - **loading** lays out text and data segments from modules into one VAS
    - **relocation** fixes relocation entries and updates addresses
  - **program loader** is a part of the OS that creates a virtual address space, loads in instructions and data from executable, resolves references to additional *shared* libraries
    - reads segments into memory, creates a stack, initializes stack pointer
    - program can then be executed by the CPU
    - *symbol tables* are used primarily for debugging

- **executable and linkable format (ELF)** is an object module format shared across different ISAs. Includes:
  - **header** with types, sizes, locations
  - **code** and **data**
  - **symbol table** for external symbols and references
  - **relocation** entries

### Subroutine Linking

- calling a subroutine:
  - *parameter passing* involves placing parameters into registers
  - *subroutine call* involves saving the *return* address on the stack, and transferring control to the entry point
  - *register saving* involves saving certain nonvolatile/callee-saved registers so that they can be restored
  - *space allocation* for local variables

- returning from a subroutine (symmetric steps):
  - place *return value*
  - pop *local* storage off stack
  - restoring *registers*
  - transfer control

- handling traps and interrupts:
  - **traps** inform software of an execution fault, **interrupts** inform software of an external event
  - similar to a procedure call, ie. have to transfer control, save state, restore state and resume process
  - different from procedure call because *hardware*-initiated, so linkage conventions are defined by the hardware
    - after event, computer state should be restored as if event never happend
  - very expensive event to handle, since the CPU is moved to a priveleged mode and new address space
- trap and interrupt *mechanism*:
  - a table associates a **program counter and processor status (PC/PS)** word pair with each possible interrupt/trap
  - when an event triggers an interrupt or trap:
    - CPU uses exception number to index into table and load a new PC/PS onto the CPU stack
    - exception continues at new PC address
      - *first level handler* saves registers, polls hardware for cause of exception, chooses and calls a *second level handler*
    - on second handler termination:
      - first level handler restores registers, reloads PC/PS, resumes execution


### Linking Libraries

- **static** libraries (linktime binding, mapped into memory at linktime):
  - library modules are directly and *permanently* embedded into the load module
  - *cons*:
    - can lead to identical **copies** of the same library code in different programs
    - difficulty keeping static libraries updated (version is *frozen*)
- **shared** libraries (linktime binding, mapped into memory at runtime):
  - reserve an address
  - linkage edit libraries into code segments
  - includes redirection table (stub library) with addresses for routines
  - at load time, libraries are *mapped* into memory
  - *pros*:
    - only single library copy required (reduced memory consumption, cached libraries)
    - version can be specified at load time
    - library changes (eg. size, new routines) easy to update
    - from client's perspective, *indistinguishable* from static libraries
  - *cons*:
    - cannot use global data storage, since other programs will use this same library copy
    - large, expensive libraries always loaded at startup
    - unlike for a static library, executable will not work on clients without the used library
- **dynamic** libraries (DLLs, runtime binding, mapped into memory during runtime):
  - libraries that are not loaded until they are actually needed
  - application asks OS to load a library into its VAS
  - application receives standard *entry points* to make calls to the DLL through
  - maintains a *table* of entry points for different DLLs
  - on DLL shutdown, application asks OS to unload module
  - loading DLLs is done through an API, but the actual loading mechanism is ABI-specific
  - *pros*:
    - runtime binding
    - libaries can be unloaded when no longer required
  - *cons*:
    - more work for the client to load and manage DLLs

## Process Virtualization
***

- the process of **virtualization** takes a *physical*, *limited* resource and creates the illusion of having *virtual*, *unlimited* copies of that resource

- the most fundamental abstraction provided by the OS to users is the **process**, or running instance of a **program**
  - a program is:
    - *static*, an abstraction stored on disk as a **load module** with resolved references
    - contains **headers**, code and data segments, **symbol table** for the linker
    - but all addresses are relative, unloaded addresses
  - a process has:
    - read-only, statically-sized **code segment** loaded into address space, contains code read in from load module
    - **data segment** containing heap that grows up loaded into address space, handles initialized, dynamic data
    - **stack segment** that grows down loaded into address space, handles procedure call stack frames
    - **stack overflow** occurs when stack and data segment meet
  - can also interpret a process as a virtual, private computer, or an *object*
    - the **state** of a process should consistently, uniquely, characterize the process
    - consists of the metadata, allocated memory, opened files, condition of an I/O operation, etc.
  - in order to run many programs at once, the OS must *virtualize* the CPU
  - OS uses a **time sharing** approach to virtualizing the CPU, as opposed to a **space sharing** approach (eg. for files)
  - there are low-level **mechanisms** that help achieve this virtualization, eg. **context switching** that allows OS to switch between running programs on a CPU
  - in addition, there are higher-level **policies** or decision algorithms used by the OS to choose which programs to run at a given time (*scheduling* policies)

- a process has an associated **machine state** or properties that it can read or write to at any given time. The machine state comprises of:
  - **memory** to store instructions and data, every process has an **address space**
    - the address space is the *virtual memory addresses* reserved for a process (illusion of infinite memory)
  - **registers** that are used during execution
    - some special registers include the **program counter** that indicates the next instruction, **stack pointer**, and **frame pointer**
  - **I/O information** for open persistent storage devices
  - other OS-related state information

### Process Overview

- conceptual process *API*:
  - *create* - OS must provide method to create new processes
    - may create a *blank* process with no initial state (Windows approach)
    - or use the *calling* process as a template (UNIX approach)
  - *destroy* - OS must provide method to destroy or kill processes
    - must clean up a terminating process:
      - reclaim resources
      - inform interprocess processes
      - remove process descriptor
  - *wait* - wait for a process to stop running
  - *misc. control* - eg. suspending and resuming processes
  - *status* - retrieve status info for a process

- process *creation* consists of:
  - *loading* code and data into memory/address space of the program
    - programs usually reside on **disk**, so the OS reads bytes from disk and places them in memory
    - modern OSs use **lazy loading** to load data only when it is needed
  - allocating and initializing the **stack** for the program (eg. with parameters, `argv`, `argc`)
  - allocating the **heap** for dynamic memory
  - initializing registers (PC, PS, SP)
  - initializing I/O (eg. opening **file descriptors**)
  - run program from its **entry point** (eg. `main`)

- **states** of a processes:
  - **running** - CPU is executing instructions for a process
  - **ready** - process is ready to run, but not currently executing
    - when a process is *scheduled*, it moves from ready state to running
    - when a process is *descheduled*, it moves from running state to ready
  - **blocked** - process has performed some operation that makes it unable to run until some other event takes place (eg. I/O request to disk)
    - the OS recognizes when a running process becomes blocked, and will run a different process to maximize time sharing
  - **initial**, **final/zombie** (not yet cleaned up, allows **parent** process to check return code)

- the OS maintains key *data structures* or **process descriptor** to track the state of processes. These include:
  - **process/task list** for all ready or running processes, another list for blocked processes
    - **process control block (PCB)** is a C structure storing information for each process
      - includes information on start and size of memory, process state (eg. scheduled, blocked) and ID, open files, CWD, context, parent
  - **register context** holds the saved registers for a *stopped* process so it can be resumed later (allows for a **context switch**)

### UNIX Process API

Using `fork()`:
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(int argc, char *argv[]) {
  printf("hello world (pid:%d)\n", (int) getpid());
  int rc = fork();
  if (rc < 0) {
    fprintf(stderr,"fork failed\n")
    exit(1);
  } else if (rc == 0) {
    /* child (new process) */
    printf("hello, I am child (pid:%d)\n", (int) getpid());
  } else {
    /* parent goes down this path (main) */
    printf("hello, I am parent of %d (pid:%d)\n",
           rc, (int) getpid());
  }
  return 0;
}
```
- `fork()` creates an almost *exact* copy of the calling process
  - to OS, there are two programs running, both about to return from `fork`
  - thus, new **child** process starts running after call to `fork`, instead of from start of `main`
    - child shares parent's code, but not its stack or data segment
    - copying large data segment can be expensive, so OS uses **copy-on-write** when needed
  - `fork` is non-deterministic, either child or parent will print first depending on the CPU **scheduler**
- new child has a copy of the address space, but the return of `fork` differs:
  - child receives return code of 0
  - parent receives new PID of child

Using `wait()`:
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/wait.h>

int main(int argc, char *argv[]){
  printf("hello world (pid:%d)\n", (int) getpid());
  int rc = fork();
  if (rc < 0) {
    fprintf(stderr,"fork failed\n")
    exit(1);
  } else if (rc == 0) {
    printf("hello, I am child (pid:%d)\n", (int) getpid());
  } else {
    int rc_wait = wait(NULL);
    printf("hello, I am parent of %d (rc_wait:%d) (pid:%d)\n",
           rc, rc_wait, (int) getpid());
  }
  return 0;
}
```
- `wait()` waits for a child process to finish executing
  - returns PID of finished child process
  - this makes code snippet deterministic, child will always print before parent in this case

Using `exec()`:
```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <sys/wait.h>

int main(int argc, char *argv[]){
  printf("hello world (pid:%d)\n", (int) getpid());
  int rc = fork();
  if (rc < 0) {
    fprintf(stderr,"fork failed\n")
    exit(1);
  } else if (rc == 0) {
    printf("hello, I am child (pid:%d)\n", (int) getpid());
    char *myargs[3];
    myargs[0] = strdup("wc");
    myargs[1] = strdup("p3.c");
    myargs[2] = NULL;
    execvp(myargs[0], myargs); /* counts words in p3.c */
    printf("this should not print");
  } else {
    int rc_wait = wait(NULL);
    printf("hello, I am parent of %d (rc_wait:%d) (pid:%d)\n",
           rc, rc_wait, (int) getpid());
  }
  return 0;
}
```
- `exec()` allows us to fork a child of a *different* program
  - does not *create* a new process, *transforms* currently running program (here, a forked child) into another running program
    - OS loads new code and overwrites current code segment, reinitializes heap and stack, runs new program
  - thus, a successful call to `exec` *never returns*

- separation of `fork()` and `exec()` have several advantages:
  - allows shell code to be run *after* `fork` and *before* `exec` and thus alter the environment of about-to-run program
  - allows for easy redirection
    - eg. in order to redirect output to some file, after `fork`, close `STDOUT`, open the file, and then `exec`
  - piping can be achieved with the `pipe()` system call

- **signals** and processes:
  - `kill()` system call can send signals to a process or **process group**, eg. `SIGINT` for interrupt, `SIGSTP` for stop
  - processes can then use `signal()` system call to catch signals

## Process Mechanisms
***

- challenges associated with **virtualizing**, ie. **time sharing** the CPU:
  - maximizing **performance** with minimal *overhead*
    - eg. enter the OS as seldom as possible
  - handling processes while retaining *control*

An initial **direct execution protocol** without limits for maximum efficiency:

| **OS**                        | **Program**                  |
|-------------------------------|------------------------------|
| create entry for process list |                              |
| allocate memory for program   |                              |
| load program into memory      |                              |
| set up stack with argc/argv   |                              |
| clear registers               |                              |
| execute `call` `main`         |                              |
|                               | run `main`                   |
|                               | execute `return` from `main` |
| free memory of process        |                              |
| remove from process list      |                              |

- this initial approach has some problems:
  - how do we ensure CPU doesn't do anything undesired or restricted, without reducing efficiency?
  - how do we efficiently switch processes in order to actually *virtualize* the CPU?

### Restricted Operations

- some operations should be **restricted** to the OS, eg. I/O or accessing more system resources
  - eg. if any process could do I/O, all data protections would be lost
  - the solution is to introcude processing modes, a restricted *user mode* and an unrestricted *kernel mode* with elevated priveleges
    - kernel mode also has full access to hardware resources
- when a *user* program wants to perform a priveleged operation, it can use a **system call**
  - system calls allow the kernel to expose important functionalities
  - a **system call number** is associated with each syscall, this indirection is a form of **protection**
  - user arguments/input must be validated by OS before performing the actual syscall in kernel mode
- to actually execute a system call:
  - the program executes a special **trap** instruction that jumps into the kernel and raises the privelege level to kernel mode
  - once in kernel, the system can perform any desired priveleged operations
  - CPU uses exception number to index into table and load a new PC/PS onto the CPU stack
  - exception continues at new PC address
    - *first level handler* saves registers, polls hardware for cause of exception, chooses and calls a *second level handler*
  - on second handler termination:
    - first level handler restores registers, reloads PC/PS, resumes execution
  - when finished, the OS calls a **return-from-trap** instruction that returns to user mode and reduces the privelege level
- nuances when executing a trap:
  - OS must push PC, flags, registers onto a per-process **kernel stack** that allows execution to be resumed by popping these values off the stack on return from trap
  - the kernel should carefully control which code executes on a trap
    - OS sets up a **trap table** at boot time that informs the hardware of the locations of **trap handlers** to call on a trap
    - note that the machine boots initially in kernel mode

An updated **limited execution protocol** to deal with system calls and traps:

| **OS** at boot        | **Hardware**                                | **Program** |
|-----------------------|---------------------------------------------|-------------|
| initialize trap table | remember addresses of trap/syscall handlers |             |

| **OS** at run (kernel mode)   | **Hardware**                   | **Program** (user mode) |
|-------------------------------|--------------------------------|-------------------------|
| create entry in process list  |                                |                         |
| allocate memory for program   |                                |                         |
| setup user stack with args    |                                |                         |
| fill kernel stack with reg/PC |                                |                         |
| return-from-trap              |                                |                         |
|                               | restore regs from kernel stack |                         |
|                               | move to user mode              |                         |
|                               | jump to `main`                   |                         |
|                               |                                | run `main`                |
|                               |                                | call syscall            |
|                               |                                | trap into OS            |
|                               | save regs to kernel stack      |                         |
|                               | move to kernel mode            |                         |
|                               | jump to trap handler           |                         |
| handle trap/syscall           |                                |                         |
| return-from-trap              |                                |                         |
|                               | restore regs from kernel stack |                         |
|                               | move to user mode              |                         |
|                               | jump to PC after trap          |                         |
|                               |                                | return from main        |
|                               |                                | trap (via `exit`)       |
| free memory of process        |                                |                         |
| remove from process list      |                                |                         |

### Process Switching

- when a program is running on a CPU, the OS is *not* running
  - how can the OS *regain control* of the CPU so that it can switch between processes?
  - in a **cooperative** scheduling system, the OS simply *waits* for a program to make a syscall or `yield` in order to regain control
    - this can lead to bugs with infinite loops or malicious programs
  - in a **non-cooperative** scheduling system, a **timer interrupt** is used
    - every so many milliseconds, an interrupt is raised automatically, and an **interrupt handler** in the OS runs
    - this timer must be started on boot up
    - the hardware must save the state of the program so that execution can resume on a return-from-trap

- once OS has control, the **scheduler** decides whether to continue running the current process (process A), or switch to a different one (process B) with a **context switch**
  - in a context switch:
    - the *hardware* saves the *user* registers for A into kernel stack A
    - the *OS* saves the *kernel* registers for A into memory in the process structure of A
    - the *OS* restores the *kernel* registers for B from memory in the process structure of B
    - the *OS* switches from kernel stack A to kernel stack B by changing the stack pointer
    - the *hardware* restores the *user* registers for B from kernel stack B
  - then, after return-from-trap, the system resumes execution of *another* process
- to deal with the issue of **concurrency**, the OS may disable interrupts for a period of time, or use locking schemes

An updated **limited execution protocol** to deal with context switching:

| **OS** at boot        | **Hardware**                                | **Program** |
|-----------------------|---------------------------------------------|-------------|
| initialize trap table | remember addresses of trap/syscall handlers |             |
| start interrupt timer | start timer                                 |             |
|                       | interrupt CPU in $X$ ms                     |             |

| **OS** at run (kernel mode)         | **Hardware**                            | **Program** (user mode) |
|-------------------------------------|-----------------------------------------|-------------------------|
|                                     |                                         | Process A               |
|                                     | timer interrupt                         |                         |
|                                     | save regs(A) $\rightarrow$ k-stack(A)   |                         |
|                                     | move to kernel mode                     |                         |
|                                     | jump to trap handler                    |                         |
| handle trap                         |                                         |                         |
| call `switch`                       |                                         |                         |
| save regs(A) into proc. struct A    |                                         |                         |
| restore regs(B) from proc. struct B |                                         |                         |
| switch to k-stack(B)                |                                         |                         |
| return-from-trap                    |                                         |                         |
|                                     | restore regs(B) $\leftarrow$ k-stack(b) |                         |
|                                     | move to user mode                       |                         |
|                                     | jump to PC of Process B                 |                         |
|                                     |                                         | Process B               |

## Process Scheduling
***

- in addition to lower level mechanisms associated with the process abstraction, OS also deals with high-level scheduling **policies** for processes
  - we will explore different scheduling algorithm approaches and how they fail when certain *assumptions* are *relaxed*:
    - every job runs for the same amount of time
    - all jobs arrive at the same time
    - each job runs to completion once started
    - all jobs only use the CPU
    - run-time of each job is known

- an initial metric for a workload is **turnaround time** of a particular job
- **first in, first out (FIFO)** scheduling:
  - schedules jobs in the order that they arrive
  - effective until first assumption is relaxed and jobs no longer run for the same amount of time...
    - issues with the **convoy effect**, where many low potential consumers may become queued *behind* a heavyweight resource consumer
- **shortest job first (SJF)** scheduling:
  - runs the shortest job first, next shortest, etc.
  - optimal until second assumption is relaxed and jobs no longer arrive at the same time...
    - shorter job could arrive while a job is still running
    - ie. SJF is a **non-preemptive** scheduler that cannot interrupt jobs
- **shortest time-to-completion first (STCF)** scheduling:
  - also relax assumption three that jobs will run to completion
  - allow scheduler to use context switching and **preempt** jobs to run another job
  - AKA **preemptive shortest job first (PSJF)**
  - effective until we consider a new metric **response time**, the time for a job to be scheduled for the first time
    - response time deals with interactivity for users
- **round robin (RR)** scheduling:
  - rather than running jobs to completion, run a job for a **time slice** before switching to the next job in the queue
  - length of time slice thus must be a multiple of the timer-interrupt period
  - tradeoff between smaller, faster time-slices and the overhead of more context switching, find good **amortization**
  - *however*, one of the worst policies in terms of turnaroud time
- when considering other systems, exploit the **overlap** of operations:
  - eg. when a process becomes blocked waiting for the completion of an I/O request, schedule another process
  - involves treating each CPU *burst* as an individual job
  - *however*, for all of the scheduling policies so far, the run-time of each job is known

### Feedback Priority Scheduling

- **multi-level feedback queue (MLFQ)** scheduling:
  - aims to optimize turnarund time *as well as* response time when assumption five is relaxed and the run-time of jobs aren't known
  - an example of a system that uses the *past* to predict the *future*
  - has a number of distinct **priority queues** with different priority levels
    - (1) If the priority of A > priority of B, only A runs
    - (2) If the priority of A = priority of B, A and B run in RR
  - the scheduler will *vary* the priorities of a job based on its *observed behavior*
    - eg. when a job repeatedly makes I/O requests to the keyboard, it will have its high priority *maintained* (interactive process)
    - eg. when a job uses the CPU intensively for long periods of time, it will have its priority *reduced* (response time isn't important)
    - (3) When a job enters the system, it is placed at the *highest* priority
    - (4a) If a job uses an entire time slice, its priority is *reduced*
    - (4b) If a job gives up CPU before time slice is up, it stays at the *same* priority
  - when a new job comes along, the scheduler *assumes* it may be a short job with a high priority:
    - if it is short, the job will run quickly and complete
    - otherwise, the job will move down the queues and run in a batch-like process
  - issues with this initial implementation:
    - with too many interactive jobs, they will consume *all* CPU time and long running jobs will be **starved**
    - a program could *maliciously* issue an I/O operation right before the end of its time slice to *always* run at a high priority
    - no mechanism for a CPU-bound job to transition to interactivity

- using **accounting**:
  - (4) Once a job uses up its time allotment at a given level, its priority is *reduced*
- using a **priority boost**:
  - in order to guarantee CPU-bound jobs will make progress against starvation, boost *all* jobs periodically
  - (5) After some time period $S$, move all jobs to the topmost queue
    - $S$ is a **voodoo constant**, if too high, starvation occurs, if too low, interact jobs would not get a proper share of the CPU

- involves many **parameters** for time slice length, number of queues, etc.
  - many implementations provide configuration files that can adjust these paramters
  - users can also give **advice** to the OS to modify scheduler behavior

### Realtime Systems

- priority scheduling is a *best effort* approach
  - there are other systems whose correctness depend on certain *timing* requirements as well as *functionality*
  - eg. space shuttle during reentry, reading sensor data at high speeds, playing media
- realtime systems are characterized by different metrics:
  - **timeliness** - how closely timing requirements are met
  - **predictability** - deviation in timeliness
- new realtime concepts:
  - **feasibility** - whether or not requirements for a task set can be met
  - **hard real-time** - strong requirements that tasks be run at specific intervals, not recoverable on failures
  - **soft real-time** - good response time required, but recoverable on failures
- realtime characteristics that make scheduling *easier*:
  - task length will be known
  - **starvation** of low priority tasks is acceptable
  - work-load may be fixed
- *differences* between ordinary time-sharing:
  - **preemption** is no longer an optimal strategy:
    - preempting running tasks will cause it to miss its deadline
    - execution time is known, so there is little need for preemption
    - real-time systems run fewer and simpler tasks, so code is not malicious or buggy (no infinite loops)

## Appendix
***

### UNIX Syscalls

- `sighandler_t signal(int signum, sighandler_t handler)` - handles signals, registers signal catchers
  - in `signal.h`
  - if `signum` is delivered to the process:
    - if `handler` is set to `SIG_IGN`, the signal is ignored
    - if `handler` is set to `SIG_DFL`, the default action occurs
    - if `handler` is set to a function, the function is called with argument `signum`
  - note that the signals `SIGKILL` and `SIGSTOP` cannot be caught or ignored
  - returns the previous value of the signal handler, or `SIG_ERR`
    - `errno` set on errors

- `int kill(pid_t pid, int sig)` - sends signals to a process
  - in `sys/types.h`, `signal.h`
  - if `pid` is positive, signal `sig` is sent to process with matching PID
  - if `pid` is 0, `sig` is sent to every process in the process group of the calling process
  - if `pid` is -1, `sig` is sent to every process possible
  - if `sig` is 0, no signal is sent, but existence and permission checks still occur
  - returns 0 on success, returns -1 and `errno` set on error
